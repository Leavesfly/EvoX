# EvoX Models LLMæ¨¡å‹é€‚é…æ¨¡å—

## ğŸ“¦ æ¨¡å—å®šä½

**å±‚çº§**: æ ¸å¿ƒå±‚ (Core Layer)  
**èŒè´£**: æä¾›ç»Ÿä¸€çš„LLMæ¨¡å‹é€‚é…å±‚,æ”¯æŒå¤šç§å¤§æ¨¡å‹æä¾›å•†  
**ä¾èµ–**: evox-core

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

evox-models ä¸º EvoX æ¡†æ¶æä¾›ç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹(LLM)æŠ½è±¡æ¥å£å’Œå¤šç§æ¨¡å‹æä¾›å•†çš„é€‚é…å®ç°,å±è”½ä¸åŒæ¨¡å‹APIçš„å·®å¼‚,è®©ä¸Šå±‚ä¸šåŠ¡æ— éœ€å…³å¿ƒå…·ä½“æ¨¡å‹å®ç°ã€‚

### æ”¯æŒçš„æ¨¡å‹æä¾›å•†

| æä¾›å•† | å®ç°ç±» | é»˜è®¤æ¨¡å‹ | ç‰¹ç‚¹ |
|--------|--------|---------|------|
| **OpenAI** | `OpenAILLM` | `gpt-4o-mini` | å®˜æ–¹æ¨¡å‹,æ€§èƒ½å¼ºå¤§ |
| **é˜¿é‡Œäº‘é€šä¹‰** | `AliyunLLM` | `qwen-plus` | å›½äº§æ¨¡å‹,ä¸­æ–‡å‹å¥½ |
| **ç¡…åŸºæµåŠ¨** | `SiliconFlowLLM` | å¯é€‰ | å¤šç§å¼€æºæ¨¡å‹,æˆæœ¬ä½ |
| **LiteLLM** | `LiteLLM` | å¯é€‰ | ç»Ÿä¸€æ¥å£,æ”¯æŒ100+æ¨¡å‹ |
| **OpenRouter** | `OpenRouterLLM` | å¯é€‰ | æ¨¡å‹è·¯ç”±,çµæ´»é€‰æ‹© |

### 1. BaseLLM æ¥å£

æ‰€æœ‰LLMå®ç°çš„ç»Ÿä¸€æ¥å£,æä¾›ä»¥ä¸‹æ ¸å¿ƒæ–¹æ³•:

```java
public interface BaseLLM {
    // åŒæ­¥ç”Ÿæˆ
    String generate(String prompt);
    String generate(List<Message> messages);
    
    // å¼‚æ­¥ç”Ÿæˆ
    Mono<String> generateAsync(String prompt);
    Mono<String> generateAsync(List<Message> messages);
    
    // æµå¼ç”Ÿæˆ
    Flux<String> stream(String prompt);
    Flux<String> stream(List<Message> messages);
    
    // é…ç½®ç®¡ç†
    LLMConfig getConfig();
}
```

### 2. LLMConfig é…ç½®ä½“ç³»

ç»Ÿä¸€çš„é…ç½®åŸºç±»,æ”¯æŒä»¥ä¸‹é€šç”¨å‚æ•°:

**åŸºç¡€é…ç½®**:
- `provider`: æä¾›å•†æ ‡è¯†
- `model`: æ¨¡å‹åç§°
- `apiKey`: APIå¯†é’¥
- `baseUrl`: APIåŸºç¡€URL

**ç”Ÿæˆå‚æ•°**:
- `temperature`: æ¸©åº¦å‚æ•°(0.0-2.0),æ§åˆ¶éšæœºæ€§
- `maxTokens`: æœ€å¤§ç”Ÿæˆtokenæ•°
- `topP`: Top-pé‡‡æ ·å‚æ•°
- `frequencyPenalty`: é¢‘ç‡æƒ©ç½š
- `presencePenalty`: å­˜åœ¨æƒ©ç½š

**é«˜çº§å‚æ•°**:
- `stream`: æ˜¯å¦å¯ç”¨æµå¼è¾“å‡º
- `timeout`: è¯·æ±‚è¶…æ—¶æ—¶é—´
- `outputResponse`: æ˜¯å¦è¾“å‡ºå“åº”åˆ°æ§åˆ¶å°

### 3. æä¾›å•†å®ç°

#### OpenAI

```java
OpenAILLMConfig config = OpenAILLMConfig.builder()
    .apiKey(System.getenv("OPENAI_API_KEY"))
    .model("gpt-4o-mini")
    .temperature(0.7f)
    .maxTokens(2000)
    .build();

OpenAILLM llm = new OpenAILLM(config);
String response = llm.generate("è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½");
```

#### é˜¿é‡Œäº‘é€šä¹‰

```java
AliyunLLMConfig config = AliyunLLMConfig.builder()
    .apiKey(System.getenv("DASHSCOPE_API_KEY"))
    .model("qwen-plus")
    .temperature(0.7f)
    .build();

AliyunLLM llm = new AliyunLLM(config);
String response = llm.generate("å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—");
```

#### ç¡…åŸºæµåŠ¨ (SiliconFlow)

æ”¯æŒå¤šç§å¼€æºæ¨¡å‹,æ€§ä»·æ¯”é«˜:

```java
SiliconFlowConfig config = SiliconFlowConfig.builder()
    .apiKey(System.getenv("SILICONFLOW_API_KEY"))
    .model("Qwen/Qwen2.5-7B-Instruct")
    .temperature(0.7f)
    .maxTokens(1000)
    .build();

SiliconFlowLLM llm = new SiliconFlowLLM(config);
String response = llm.generate("ç¼–å†™ä¸€ä¸ªPythonå¿«é€Ÿæ’åº");
```

**æ”¯æŒçš„æ¨¡å‹ç¤ºä¾‹**:
- Qwenç³»åˆ—: `Qwen/Qwen2.5-7B-Instruct`
- DeepSeekç³»åˆ—: `deepseek-ai/DeepSeek-V2.5`
- å…¶ä»–å¼€æºæ¨¡å‹...

#### LiteLLM (ç»Ÿä¸€æ¥å£)

é€šè¿‡LiteLLM Proxyè®¿é—®100+æ¨¡å‹:

```java
LiteLLMConfig config = LiteLLMConfig.builder()
    .litellmBaseUrl("http://localhost:4000")
    .model("gpt-4o-mini")
    .openaiKey(System.getenv("OPENAI_API_KEY"))
    .anthropicKey(System.getenv("ANTHROPIC_API_KEY"))
    .build();

LiteLLM llm = new LiteLLM(config);
```

**æ”¯æŒçš„æä¾›å•†**:
- OpenAI (gpt-4, gpt-3.5-turbo, ...)
- Anthropic (claude-3-5-sonnet, ...)
- Google (gemini-pro, ...)
- DeepSeek, Groq, Perplexity ...

### 4. æµå¼è¾“å‡º

æ‰€æœ‰LLMå®ç°éƒ½æ”¯æŒæµå¼è¾“å‡º:

```java
Flux<String> stream = llm.stream("è®²ä¸€ä¸ªæœ‰è¶£çš„æ•…äº‹");
stream.subscribe(
    chunk -> System.out.print(chunk),  // å¤„ç†æ¯ä¸ªchunk
    error -> log.error("Error", error),
    () -> System.out.println("\nå®Œæˆ")
);
```

### 5. å¼‚æ­¥è°ƒç”¨

åŸºäº Reactor çš„å“åº”å¼ç¼–ç¨‹æ”¯æŒ:

```java
Mono<String> async = llm.generateAsync("åˆ†æå¸‚åœºè¶‹åŠ¿");
async.subscribe(
    result -> log.info("ç»“æœ: {}", result),
    error -> log.error("é”™è¯¯", error)
);
```

## ğŸ“‚ ç›®å½•ç»“æ„

```
evox-models/
â”œâ”€â”€ base/                   # åŸºç¡€æ¥å£
â”‚   â””â”€â”€ BaseLLM.java
â”œâ”€â”€ config/                 # é…ç½®ç±»
â”‚   â”œâ”€â”€ LLMConfig.java      # é…ç½®åŸºç±»
â”‚   â”œâ”€â”€ OpenAILLMConfig.java
â”‚   â”œâ”€â”€ AliyunLLMConfig.java
â”‚   â”œâ”€â”€ SiliconFlowConfig.java
â”‚   â”œâ”€â”€ LiteLLMConfig.java
â”‚   â””â”€â”€ OpenRouterConfig.java
â”œâ”€â”€ openai/                 # OpenAIå®ç°
â”‚   â””â”€â”€ OpenAILLM.java
â”œâ”€â”€ aliyun/                 # é˜¿é‡Œäº‘å®ç°
â”‚   â””â”€â”€ AliyunLLM.java
â”œâ”€â”€ siliconflow/            # ç¡…åŸºæµåŠ¨å®ç°
â”‚   â”œâ”€â”€ SiliconFlowLLM.java
â”‚   â””â”€â”€ SiliconFlowModel.java
â”œâ”€â”€ litellm/                # LiteLLMå®ç°
â”‚   â””â”€â”€ LiteLLM.java
â””â”€â”€ openrouter/             # OpenRouterå®ç°
    â””â”€â”€ OpenRouterLLM.java
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### Maven ä¾èµ–

```xml
<dependency>
    <groupId>io.leavesfly.evox</groupId>
    <artifactId>evox-models</artifactId>
    <version>1.0.0-SNAPSHOT</version>
</dependency>
```

### Spring Boot è‡ªåŠ¨é…ç½®

åœ¨ `application.yml` ä¸­é…ç½®:

```yaml
evox:
  llm:
    provider: openai              # æˆ– dashscope, litellm, siliconflow
    api-key: ${OPENAI_API_KEY}
    model: gpt-4o-mini
    temperature: 0.7
    max-tokens: 2000
    timeout: 30000
```

ç„¶åç›´æ¥æ³¨å…¥ä½¿ç”¨:

```java
@Autowired
private BaseLLM llm;

public String chat(String input) {
    return llm.generate(input);
}
```

### ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶:

```bash
# OpenAI
OPENAI_API_KEY=sk-your-api-key
OPENAI_MODEL=gpt-4o-mini

# é˜¿é‡Œäº‘é€šä¹‰
DASHSCOPE_API_KEY=your-dashscope-key
DASHSCOPE_MODEL=qwen-plus

# ç¡…åŸºæµåŠ¨
SILICONFLOW_API_KEY=your-siliconflow-key

# LiteLLM
LITELLM_API_KEY=your-api-key
LITELLM_BASE_URL=http://localhost:4000
```

### ç¼–ç¨‹å¼åˆ›å»º

```java
// æ–¹å¼1: ä½¿ç”¨Builder
OpenAILLMConfig config = OpenAILLMConfig.builder()
    .apiKey("sk-xxx")
    .model("gpt-4o-mini")
    .temperature(0.7f)
    .maxTokens(2000)
    .build();
    
OpenAILLM llm = new OpenAILLM(config);

// æ–¹å¼2: é…ç½®å¯¹è±¡
LLMConfig config = new OpenAILLMConfig();
config.setApiKey("sk-xxx");
config.setModel("gpt-4o-mini");
config.setTemperature(0.7f);

BaseLLM llm = new OpenAILLM((OpenAILLMConfig) config);
```

## ğŸ’¡ é«˜çº§ç”¨æ³•

### 1. æ¶ˆæ¯å†å²å¯¹è¯

```java
List<Message> messages = List.of(
    Message.builder()
        .content("ä½ æ˜¯ä¸€ä¸ªPythonä¸“å®¶")
        .messageType(MessageType.SYSTEM)
        .build(),
    Message.builder()
        .content("å¦‚ä½•å®ç°å¿«é€Ÿæ’åº?")
        .messageType(MessageType.INPUT)
        .build()
);

String response = llm.generate(messages);
```

### 2. è‡ªå®šä¹‰å‚æ•°

```java
OpenAILLMConfig config = OpenAILLMConfig.builder()
    .apiKey("sk-xxx")
    .model("gpt-4o")
    .temperature(0.9f)           // æ›´é«˜çš„åˆ›é€ æ€§
    .maxTokens(4000)             // æ›´é•¿çš„è¾“å‡º
    .topP(0.95f)                 // Top-pé‡‡æ ·
    .frequencyPenalty(0.5f)      // é¢‘ç‡æƒ©ç½š
    .presencePenalty(0.5f)       // å­˜åœ¨æƒ©ç½š
    .build();
```

### 3. åˆ‡æ¢æ¨¡å‹

```java
// å¼€å‘ç¯å¢ƒ: ä½¿ç”¨å¿«é€Ÿä¾¿å®œçš„æ¨¡å‹
BaseLLM devLLM = new OpenAILLM(
    OpenAILLMConfig.builder()
        .model("gpt-4o-mini")
        .build()
);

// ç”Ÿäº§ç¯å¢ƒ: ä½¿ç”¨æ›´å¼ºå¤§çš„æ¨¡å‹
BaseLLM prodLLM = new OpenAILLM(
    OpenAILLMConfig.builder()
        .model("gpt-4o")
        .build()
);
```

### 4. ç»Ÿä¸€å¤šæ¨¡å‹è®¿é—®

ä½¿ç”¨ LiteLLM ç»Ÿä¸€è®¿é—®å¤šä¸ªæä¾›å•†:

```java
LiteLLMConfig config = LiteLLMConfig.builder()
    .litellmBaseUrl("http://localhost:4000")
    .model("gpt-4o-mini")          // OpenAI
    // .model("claude-3-5-sonnet")  // Anthropic
    // .model("gemini-pro")         // Google
    .build();
```

## ğŸ“ è®¾è®¡åŸåˆ™

- **ç»Ÿä¸€æŠ½è±¡**: BaseLLMæ¥å£å±è”½ä¸åŒæ¨¡å‹å·®å¼‚
- **çµæ´»é…ç½®**: æ”¯æŒä»£ç é…ç½®å’Œç¯å¢ƒå˜é‡é…ç½®
- **å“åº”å¼ç¼–ç¨‹**: åŸºäºReactor,æ”¯æŒå¼‚æ­¥å’Œæµå¼
- **å¯æ‰©å±•æ€§**: æ˜“äºæ·»åŠ æ–°çš„æ¨¡å‹æä¾›å•†

## ğŸ“Š é€‚ç”¨åœºæ™¯

- æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ
- æ–‡æœ¬ç”Ÿæˆå’Œæ”¹å†™
- ä»£ç ç”Ÿæˆå’Œè§£é‡Š
- çŸ¥è¯†é—®ç­”
- å†…å®¹æ‘˜è¦å’Œç¿»è¯‘
- Promptä¼˜åŒ–å’Œæµ‹è¯•

## ğŸ”— ç›¸å…³æ¨¡å—

- **evox-core**: æä¾›åŸºç¡€æŠ½è±¡å’Œæ¶ˆæ¯ç³»ç»Ÿ
- **evox-actions**: ä½¿ç”¨LLMæ‰§è¡Œå„ç§åŠ¨ä½œ
- **evox-agents**: Agentä½¿ç”¨LLMè¿›è¡Œæ¨ç†å’Œå†³ç­–
- **evox-optimizers**: ä¼˜åŒ–Promptå’Œæ¨¡å‹å‚æ•°

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **APIå¯†é’¥å®‰å…¨**: ä¸è¦å°†APIå¯†é’¥ç¡¬ç¼–ç ,ä½¿ç”¨ç¯å¢ƒå˜é‡
2. **æˆæœ¬æ§åˆ¶**: è®¾ç½®åˆç†çš„`maxTokens`,é¿å…è¿‡åº¦æ¶ˆè€—
3. **è¶…æ—¶è®¾ç½®**: æ ¹æ®ä¸šåŠ¡åœºæ™¯è®¾ç½®åˆç†çš„`timeout`
4. **é”™è¯¯å¤„ç†**: æ•è·å¹¶å¤„ç†LLMè°ƒç”¨å¼‚å¸¸
5. **é€Ÿç‡é™åˆ¶**: æ³¨æ„å„æä¾›å•†çš„APIé€Ÿç‡é™åˆ¶
